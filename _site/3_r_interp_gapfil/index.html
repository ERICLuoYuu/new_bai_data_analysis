<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>  Interpolation and Gap Filling - Data Analysis for Ecologists</title>
<meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.">


  <meta name="author" content="Nicolas Behrens">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Data Analysis for Ecologists">
<meta property="og:title" content="  Interpolation and Gap Filling">
<meta property="og:url" content="http://localhost:4000/3_r_interp_gapfil/">


  <meta property="og:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.">












<link rel="canonical" href="http://localhost:4000/3_r_interp_gapfil/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Data Analysis for Ecologists Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Data Analysis for Ecologists
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/python_0_landing_page">Python for Ecologists</a>
            </li><li class="masthead__menu-item">
              <a href="/0_r_landing_page/">R for Ecologists</a>
            </li><li class="masthead__menu-item">
              <a href="/about">About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/bio-photo.jpg" alt="Nicolas Behrens" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Nicolas Behrens</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>I am a Geoscientist, Java developer and now Ecology PhD-candidate. I love programming, working with time series data and being in my beautiful peatland research sites where I investigate controls on greenhouse gas emissions from peat soils.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://github.com/nicbehr" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          <a href="/1_r_basics/"><span class="nav__sub-title">Basics of R</span></a>
        

        
      </li>
    
      <li>
        
          <a href="/2_r_data_viz/"><span class="nav__sub-title">Data Visualization</span></a>
        

        
      </li>
    
      <li>
        
          <a href="/3_r_interp_gapfil/"><span class="nav__sub-title">Interpolation and Gap Filling</span></a>
        

        
      </li>
    
      <li>
        
          <a href="/4_r_extreme_detection/"><span class="nav__sub-title">Extreme Value Detection</span></a>
        

        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="  Interpolation and Gap Filling">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/3_r_interp_gapfil/" class="u-url" itemprop="url"><ol>
  <li>Interpolation and Gap Filling</li>
</ol>
</a>
          </h1>
          


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
        <p>In this exercise we look at interpolation and the rather time-series specific topic of data-gap filling.<br />
In time-series data we often have gaps due to a variety of reasons. They can result from instrumental issues or maintenance times or unfavorable weather conditions which leads to data being discarded. These data gaps can be filled with statistical methods.</p>

<p>In this lesson exercises are not completely separated from the content. Just follow along, grab the code and in some parts you will get snippets to run and fiddle with yourself.</p>

<p>First load up the libraries we will need:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>Before we start plotting data we will see, how we can deal with missing values which are already handled by the institution
meaasuring the data, e.g. the DWD</p>

<h3 id="1-loading-and--converting-data">1. Loading and  converting data:</h3>
<p>First we read in data:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_site</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"data_site.csv"</span><span class="p">)</span><span class="w">
</span><span class="n">data_dwd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"data_dwd.csv"</span><span class="p">)</span><span class="w">
</span><span class="n">data_dwd</span><span class="o">$</span><span class="n">datetime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.Date</span><span class="p">(</span><span class="n">data_dwd</span><span class="o">$</span><span class="n">datetime</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>Now we will create some missing data with a placeholder value of -888.88 as an example:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_dwd_with_placeholders</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_dwd</span><span class="w">
</span><span class="n">data_dwd_with_placeholders</span><span class="o">$</span><span class="n">RR</span><span class="p">[</span><span class="m">250</span><span class="o">:</span><span class="m">350</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">-888.88</span><span class="w">
</span></code></pre></div></div>

<p>In this example you see how this data gets plotted. Autoformat of the axes is not working and the data
is barely readable.</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bad_fig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">data_dwd_with_placeholders</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">datetime</span><span class="p">,</span><span class="w"> </span><span class="n">RR</span><span class="p">))</span><span class="o">+</span><span class="w">
    </span><span class="n">geom_line</span><span class="p">()</span><span class="o">+</span><span class="w">
    </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"Precipipitation [mm]"</span><span class="p">)</span><span class="o">+</span><span class="w">
    </span><span class="n">scale_x_date</span><span class="p">(</span><span class="n">date_breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"12 months"</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">date_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"%y-%m"</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">bad_fig</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>lets look at the first way to solve this.
first we identify the problematic values, e.g. by searching for unrealistic values:</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_dwd_with_placeholders</span><span class="p">[</span><span class="n">data_dwd_with_placeholders</span><span class="o">$</span><span class="n">RR</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="m">-500</span><span class="p">,]</span><span class="w">
</span></code></pre></div></div>
<p>now that we know, that the placeholder values is -888.88 we can find all indices where the column contains this value:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">indices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_dwd_with_placeholders</span><span class="o">$</span><span class="n">RR</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">-888.88</span><span class="w">
</span></code></pre></div></div>
<p>we can use these indices to easily replace these values by NA</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_dwd_with_placeholders</span><span class="o">$</span><span class="n">RR</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA</span><span class="w">
</span></code></pre></div></div>

<p>To show that the line plot now automatically does not draw this gap, we slice some rows containing the 
missing values from the placeholder dateset:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">slice_data_dwd_with_placeholders</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_dwd_with_placeholders</span><span class="p">[</span><span class="m">200</span><span class="o">:</span><span class="m">400</span><span class="p">,]</span><span class="w">
</span></code></pre></div></div>
<p>Now you can see that the autoformatting of the axes works and the data is left out when plotting:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">better_fig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">slice_data_dwd_with_placeholders</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">datetime</span><span class="p">,</span><span class="w"> </span><span class="n">RR</span><span class="p">))</span><span class="o">+</span><span class="w">
    </span><span class="n">geom_line</span><span class="p">()</span><span class="o">+</span><span class="w">
    </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Date [yy-mm-dd]"</span><span class="p">)</span><span class="o">+</span><span class="w">
    </span><span class="n">ylab</span><span class="p">(</span><span class="s2">"Precipipitation [mm]"</span><span class="p">)</span><span class="o">+</span><span class="w">
    </span><span class="n">scale_x_date</span><span class="p">(</span><span class="n">date_breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"1 months"</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">date_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"%y-%m-%d"</span><span class="p">)</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">better_fig</span><span class="p">)</span><span class="w">

</span></code></pre></div></div>

<h3 id="2-gap-filling-interpolation-and-modelling">2. Gap Filling, interpolation and modelling</h3>

<p>In the next part we will discuss how we can work with timeseries that have gaps of different sizes.
This is a regular task when working with long-time observations and there are a couple of options,
depending on what data is available to you and what is the final evaluation goal you have in mind.</p>

<h4 id="21-simple-linear-interpolation">2.1: Simple linear interpolation</h4>

<p>You do basic interpolation in your every day live. You want to bake a cake and only find a receipe for an 8 person cake,
but only 3 friends are coming over for cake time. In the receipe you have to use 1 kg of flour. Intuitively you can 
see that since you will only be four at the table, you can alter the receipe and only use 500 g of flour.
And already did you do some interpolation! 
What you easily did right away in your head could be mathematically formulated as:
y = 125 * x
where y is the amount of flour in grams and x is the number of people eating cake.</p>

<p>The formula for an interpolation between two points (x1,y1) and (x2,y2) at a specific point
(xn, yn) is:</p>

<div> $$ yn = y1 + \frac{(y_{2}-y_{1})}{(x_{2}-x_{1})} * (x_{n} - x_{1}) $$ </div>

<p>We simply construct a straight line where y1 is our y-intercept, the slope is derived 
from the two points with the well known slope-formula</p>

<div> $$ m = (y2-y1)/(x2-x1) $$ </div>

<p>and our x value on this constructed line is difference between the point we want to look at minus the starting point</p>

<p>Note that in this form of y = mx + b we only have one x which we use to explain our y-value. We have one “predictor”.
Using only one predictor gives us a so called simple linear regression. This is a super simple form of interpolation 
and of course leaves a lot of information aside.</p>

<p>Lets look at a simple example of how to actually do linear interpolation in R:</p>

<p>First we create a dataset of x and y values which represents the true values</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">13</span><span class="p">,</span><span class="m">4</span><span class="p">,</span><span class="m">10</span><span class="p">,</span><span class="m">19</span><span class="p">,</span><span class="m">15</span><span class="p">,</span><span class="m">13</span><span class="p">,</span><span class="m">21</span><span class="p">,</span><span class="m">27</span><span class="p">)</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">,</span><span class="m">4</span><span class="p">,</span><span class="m">5</span><span class="p">,</span><span class="m">6</span><span class="p">,</span><span class="m">7</span><span class="p">,</span><span class="m">8</span><span class="p">,</span><span class="m">9</span><span class="p">,</span><span class="m">10</span><span class="p">,</span><span class="m">11</span><span class="p">)</span><span class="w">
</span><span class="n">data_full</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
    </span><span class="n">x</span><span class="p">,</span><span class="w">
    </span><span class="n">y</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>Now we build a second dataset in which we leave singular points out. This would in a real scenario be the data that 
we actually have</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="kc">NA</span><span class="p">,</span><span class="m">4</span><span class="p">,</span><span class="kc">NA</span><span class="p">,</span><span class="m">19</span><span class="p">,</span><span class="m">15</span><span class="p">,</span><span class="m">13</span><span class="p">,</span><span class="kc">NA</span><span class="p">,</span><span class="m">27</span><span class="p">)</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">,</span><span class="m">4</span><span class="p">,</span><span class="m">5</span><span class="p">,</span><span class="m">6</span><span class="p">,</span><span class="m">7</span><span class="p">,</span><span class="m">8</span><span class="p">,</span><span class="m">9</span><span class="p">,</span><span class="m">10</span><span class="p">,</span><span class="m">11</span><span class="p">)</span><span class="w">

</span><span class="n">deleted_indices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">6</span><span class="p">,</span><span class="m">10</span><span class="p">)</span><span class="w">

</span><span class="n">data_reduced</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
    </span><span class="n">x</span><span class="p">,</span><span class="w">
    </span><span class="n">y</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>lets take a quick look at the two datasets:</p>

<p>First we create as imple line plot of the reduced data</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">data_full</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data_full</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>Now we add the missing values in red to the plot.
We can make these points red using the “col” argument for “color” and define the symbols with the “pch” argument.
You can find a list of all available symbols e.g. here:<br />
<a href="http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r">Link: sthda.com point shapes</a></p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">points</span><span class="p">(</span><span class="n">data_reduced</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data_reduced</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>Finally we add a legend to the plot so we know what is what. We can use the inbuilt method “legend()”
The first two arguments 1 and 28 are the x and y positions of the legend. To the argument “legend” we pass
a vector of strings, which is the legend text. Then we can again use the col and pch arguments to define the 
colors and symbols of the legend according to our plot.</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">legend</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">28</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"original data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"deleted data"</span><span class="p">),</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">19</span><span class="p">,</span><span class="m">19</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>To do a linear interpolation between each adjacent points you can use a simple inbuilt function in R, the approx()
function:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">approx</span><span class="p">(</span><span class="n">data_reduced</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data_reduced</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"linear"</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="o">=</span><span class="m">11</span><span class="p">)</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">data_full</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data_full</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">)</span><span class="w">
</span><span class="n">points</span><span class="p">(</span><span class="n">data_reduced</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data_reduced</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>Since we can use the approx() function to create a pretty much infinite nubmer of approximations between the 
points we will use a line to visualize it. We can do that with the lines() function and pass it our predicitons:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lines</span><span class="p">(</span><span class="n">prediction</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">prediction</span><span class="o">$</span><span class="n">y</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>Now we add a legend again. To specify the line type in the legend we can use “lty” argument. Note that 
we have to keep the order of entries from our “legend” keyword and add an entry in pch and lty for each legend
entry. For the point data we add “NA” in index 1 and 2 in lty, and in pch we add “NA” in the third position,
as that is the line and not a point.</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">legend</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">28</span><span class="p">,</span><span class="w"> 
       </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"deleted data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"remaining data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"interpolation line"</span><span class="p">),</span><span class="w"> 
       </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">,</span><span class="w"> </span><span class="s2">"black"</span><span class="p">),</span><span class="w"> 
       </span><span class="n">lty</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="kc">NA</span><span class="p">,</span><span class="m">1</span><span class="p">),</span><span class="w"> 
       </span><span class="n">pch</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">16</span><span class="p">,</span><span class="w"> </span><span class="kc">NA</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>In this approach all we did was to draw straight lines between adjacent points. As you see, for the first point
the prediction was rather poor, the other two where pretty well reconstructed.
However, with this approach we leave all the information the other points give us about the data aside. 
Imagine for example that you have a timeseries where you measure temperature at midnight and at 12AM.
If one datapoint was missing, you would connect the two night time temperatures and interpolate the daytime
temperature way off.</p>

<p>A simple measure of how well our model performed is to look at the residual standard error. We calculate it
as</p>

<div> $$ \sqrt{\frac{\sum_{i=1}^n (y[i] - ypredicted[i])^2}{df}} $$ </div>

<p>where y is the true value, ypredicted is the predicted y value, and df is the degrees of freedom. Df is the total
number of observations used for the model fitting minus the number of model parameters. Since we have 11 total 
data points of which 3 are missing and we have 2 model parameter we have 6 degrees of freedom.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">RSE_linerp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">sum</span><span class="p">((</span><span class="n">data_full</span><span class="o">$</span><span class="n">y</span><span class="p">[</span><span class="n">deleted_indices</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">prediction</span><span class="o">$</span><span class="n">y</span><span class="p">[</span><span class="n">deleted_indices</span><span class="p">])</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="m">6</span><span class="p">)</span><span class="w">
</span><span class="n">RSE_linerp</span><span class="w">
</span></code></pre></div></div>

<h4 id="22-simple-linear-models">2.2: Simple linear models</h4>

<p>Another approach would be to create a linear model that builds not only on the two points but rather the whole of the 
dataset that we have available.</p>

<p>So what we want to achieve, is to find a function that constructs our unknown data points based on the data we have
availabel in the best possible way. That means, that we want to have as little errors in our model as possible. 
The error is usually measured as the “sum of squared errors” (SSE) which is the distance between a true value 
and the predicted value. We square it to avoid negative and positive values counterbalancing each other.</p>

<p>Looking at an array of n data points we can write</p>

<div> $$ SSE = \sum_{i=1}^n (y(i) - b - m * x(i))^2 $$ </div>

<p>y(i) is the true y value at the predicted point, b is the y-intercept of the linear model, 
m is the first coefficient of the linear model and x(i) is the x-value at the predicted point.</p>

<p>Since we want to find the straight line, that MINIMIZES the SSE, we call a procedure like this
a “minimization problem” and specifically the estimation of this line is called a “least squares estimation”.</p>

<p>In the easiest way of fitting a linear model to such a dataset, it all depends on the mean of our dataset.
To derive the model parameters we can use the following relations where we replace b with alpha and m with beta
(as that is the general standard). Also we will now denote the predicted y-value with a ^ on top of that, which is
the common standard in literature. Sometimes this is also referred to as y_hat.</p>

<div> $$ \hat{y}_{i} = \alpha + \beta * x_{i} $$ </div>

<div> $$ \alpha = \bar{y} - (m \bar{x}) $$ </div>

<div> $$ \beta = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} $$ </div>

<p>If we would do it by hand, we would simply plug in all the numbers we have into the expression for beta and
use the result to derive our alpha</p>

<p>Luckily, we have inbuilt functions in R for that.
To fit a linear model to the data we have in the reduced dataset we can use a simple inbuilt method:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">data_reduced</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">data_reduced</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_reduced</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>We provide the dataset that has the information to the “data” argument and specify, which column is supposed
to be explained by which other column (or which column is dependent on which other column) with data_reduced$y ~ data_reduced$x.</p>

<p>When we print the model we see the paramters of our linear line, that R has fitted for us:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">$</span><span class="n">coefficients</span><span class="w">
</span></code></pre></div></div>

<p>So we  want to have y-values corresponding to the x-indices of 3, 6 and 10
We compute them by passing the x values at these indices (which are just the indices in this case) to the function
of our linear model:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_hat_manual</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2.528</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">deleted_indices</span><span class="w"> </span><span class="m">-4.411</span><span class="w">
</span><span class="n">y_hat_manual</span><span class="w">
</span></code></pre></div></div>

<p>The same effect can be achieved by using the “predict” function and passing it our model as well as the dataframe
with the missing values</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_hat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_reduced</span><span class="p">)</span><span class="w"> </span><span class="c1">#Predictions on Testing data</span><span class="w">
</span><span class="n">y_hat</span><span class="p">[</span><span class="n">deleted_indices</span><span class="p">]</span><span class="w">
</span></code></pre></div></div>

<p>We can look at out interpolated values by plotting them as red dots together with our reduced dataset:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">data_reduced</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data_reduced</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">)</span><span class="w">
</span><span class="n">points</span><span class="p">(</span><span class="n">deleted_indices</span><span class="p">,</span><span class="w"> </span><span class="n">y_hat</span><span class="p">[</span><span class="n">deleted_indices</span><span class="p">],</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span><span class="w">
</span><span class="n">legend</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">28</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"original data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"modelled data"</span><span class="p">),</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">19</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>We can visualize the predicted values of our model as a line by displaying the first and last values of our
vector as a line plot:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">data_reduced</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data_reduced</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">)</span><span class="w">
</span><span class="n">points</span><span class="p">(</span><span class="n">deleted_indices</span><span class="p">,</span><span class="w"> </span><span class="n">y_hat</span><span class="p">[</span><span class="n">deleted_indices</span><span class="p">],</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span><span class="w">
</span><span class="n">points</span><span class="p">(</span><span class="n">deleted_indices</span><span class="p">,</span><span class="w"> </span><span class="n">data_full</span><span class="o">$</span><span class="n">y</span><span class="p">[</span><span class="n">deleted_indices</span><span class="p">],</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">11</span><span class="p">),</span><span class="n">y_hat</span><span class="p">[</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">11</span><span class="p">)])</span><span class="w">
</span><span class="n">legend</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">28</span><span class="p">,</span><span class="w"> 
       </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"reduced data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"modelled data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"true data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"regression line"</span><span class="p">),</span><span class="w"> 
       </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">,</span><span class="w"> </span><span class="s2">"black"</span><span class="p">),</span><span class="w"> 
       </span><span class="n">lty</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="kc">NA</span><span class="p">,</span><span class="kc">NA</span><span class="p">,</span><span class="m">1</span><span class="p">),</span><span class="w"> 
       </span><span class="n">pch</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">16</span><span class="p">,</span><span class="m">16</span><span class="p">,</span><span class="kc">NA</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>Finally we need to look at statistical metrics to find out, how well our linear model performed.
Luckily we can easily get a comprehensive overview of statistical metrics in R by calling the summary() 
function on the linear model:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>A few things we can take from this are:<br />
<strong>a)</strong> The adjusted R^2 is 0.8076, which is the ratio of the sum of squared errors divided by the sum of squared
deviations from the mean. You can say that r_squared is a measure of how much of the variance in the original
data is reflected by the model. In this case, as our model is just a line, the amount of variance captured in the model
and stems from the linear trend that is inherent in the original data.</p>

<p>Whether an R^2 is good or bad depends heavily on the application. If you are a social scientist and work on 
voter behaviour an r_square of 0.65 may be spectacularly good. If you want to calibrate your measurement device
and the reference and measured values have an r_square of less than 0.85 you might want to check it again…</p>

<p><strong>b)</strong> The residual standard error is the square root of the sum of squared errors divided by the degrees of freedom. 
In plain words you could say that it shows how much you can expect the model to be wrong on average. So when
we apply our model we have to take care not to overinteprete small changes in our modelled data.</p>

<p>We wont go much deeper into statistical metrics here. But as you can see, this model does represent certain characteristics
of the data regarding its variance (judging by the rsquare of &gt; 0.8)
but has a pretty high average error of more than 4 while we are in a 
domain of data that only reaches from 1 to 27.</p>

<h3 id="part-23-multiple-linear-models">Part 2.3: Multiple linear models</h3>

<p>Lets look at another way to make our models a bit more flexible
With lm() we created a linear model with only one parameter. Obviously that did not catch all of the variance in our 
data. In reality we often have more variables at hand which can help us explain the measure of interest.
For example to fill gaps in temperature data instead of using only the time of day, we could add variables
such as the incoming radiation or the relative humidity of the air.</p>

<p>First lets load in some actual data and aggregate it to daily values:
For the aggregation we can use the same function we used in the previous lecture:</p>

<p>First we make a copy of the original dataframe to a new variabl to work non-destructive (meaning, we keep the original
data intact)</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_site_daily</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_site</span><span class="w">
</span></code></pre></div></div>

<p>First we set the datetime column in our copied dataframe to a “Date” type. That gets rid of the time information and 
only leaves us with yyyy-mm-dd information</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_site_daily</span><span class="o">$</span><span class="n">datetime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.Date</span><span class="p">(</span><span class="n">data_site_daily</span><span class="o">$</span><span class="n">datetime</span><span class="p">)</span><span class="w">
</span><span class="n">data_site_daily</span><span class="w">

</span></code></pre></div></div>

<p>Now we can use a dplyr pipeline to do the actual resampling of the data:
We insert the dataframe with daily values into the pipeline with the %&gt;% operator</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_site_daily</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_site_daily</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
</span></code></pre></div></div>
<p>Now we use the group_by() function to grab all the data that belongs to each day</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="n">group_by</span><span class="p">(</span><span class="n">datetime</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
</span></code></pre></div></div>
<p>Then we use another dplyr function, summarize_if() to aggregate the data. To the function we pass two arguments:</p>
<ol>
  <li>we say that we only want to summarize if the data is numeric. Thereby we prevent that e.g. the date column is summarized as well</li>
  <li>We pass an argument that defines, HOW we want to aggregate the data. In this case “mean”, taking the average
for each day as the new value
    <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="n">dplyr</span><span class="o">::</span><span class="n">summarize_if</span><span class="p">(</span><span class="n">is.numeric</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
</span></code></pre></div>    </div>
    <p>Finally we specify the datatype of our output, which is a data.frame</p>
    <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w"> </span><span class="n">as.data.frame</span><span class="p">()</span><span class="w">
</span></code></pre></div>    </div>
    <p>Just like before, for precipitation we do not want the data to be averaged but rather summarized.
We do the exact same pipeline as above, except this time we use th dplyr summarize() function instead of the 
summarize_if() function and pass it the sum() function as the method of aggregation.</p>
    <div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">site_precip_daily</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_site_daily</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">group_by</span><span class="p">(</span><span class="n">datetime</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">dplyr</span><span class="o">::</span><span class="n">summarize</span><span class="p">(</span><span class="n">RR</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">RR</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">as.data.frame</span><span class="p">()</span><span class="w">
</span><span class="n">data_site_daily</span><span class="o">$</span><span class="n">RR</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">site_precip_daily</span><span class="o">$</span><span class="n">RR</span><span class="w">
</span></code></pre></div>    </div>
  </li>
</ol>

<p>Once again I replace some values by NA to create a gap in the data for temperature and also for the dewpoint temperature, as that is calculated from air temperature.</p>

<p>First we define a range of values to delete:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">removed_indices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">320</span><span class="o">:</span><span class="m">360</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
<p>Now again we create a dataset as a copy of the original to not temper with the original data:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_site_daily_reduced</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_site_daily</span><span class="w">
</span></code></pre></div></div>
<p>Now we set the values in our copied dataframe for temperature and dewpoint temperature at the specified indices
in removed_indices to NA</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_site_daily_reduced</span><span class="p">[</span><span class="n">removed_indices</span><span class="p">,]</span><span class="o">$</span><span class="nb">T</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA</span><span class="w">
</span><span class="n">data_site_daily_reduced</span><span class="p">[</span><span class="n">removed_indices</span><span class="p">,]</span><span class="o">$</span><span class="n">DP</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NA</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">data_site_daily_reduced</span><span class="o">$</span><span class="nb">T</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="n">cex</span><span class="o">=</span><span class="m">.5</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"Day"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"T [°C]"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>When we try to simply interpolate with the pointwise linear interpolation, 
you will see that we get a pretty uninformed output:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">approx</span><span class="p">(</span><span class="n">data_site_daily_reduced</span><span class="o">$</span><span class="nb">T</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span><span class="w">

</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="o">$</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="o">$</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">,</span><span class="w"> </span><span class="n">cex</span><span class="o">=</span><span class="m">.5</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"Day"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"T [°C]"</span><span class="p">)</span><span class="w">
</span><span class="n">points</span><span class="p">(</span><span class="n">data_site_daily_reduced</span><span class="o">$</span><span class="nb">T</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"black"</span><span class="p">,</span><span class="n">cex</span><span class="o">=</span><span class="m">.5</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">)</span><span class="w">
</span><span class="n">legend</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="m">25</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"true data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"modelled data"</span><span class="p">),</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">19</span><span class="p">,</span><span class="m">19</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>Once again we will create a model to reconstruct our missing data. However, this time we have a whole dataset
of predictors to choose from.<br />
Since we want to fill a gap in temperature data, we need to find predictors that are well correlated with 
temperature. To figure out which ones are suitable we can make use of the correlation matrix.<br />
A correlation matrix is a normalized form of a covariance matrix. The values vary between -1 and 1. 
A value of 1 signals a perfect positive, -1 a perfect negative correlation. 0 means that the two variables are not
correlated at all.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">data_site_daily_reduced</span><span class="p">[,</span><span class="m">3</span><span class="o">:</span><span class="m">18</span><span class="p">],</span><span class="w">  </span><span class="n">use</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"complete.obs"</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="o">=</span><span class="s2">"pearson"</span><span class="p">)</span><span class="w">
</span><span class="n">cor</span><span class="w">

</span></code></pre></div></div>

<p>As you can see in the output of the code above, all the values on the diagonal are 1. Variables with themselves
are perfectly correlated.</p>

<p>Try to figure out, which variables could be suitable to fill the gaps in our data from the below table.
But beware: DP is the dewpoint temperature, which is calculated from the air temperature. So for the gaps
you will not have that data available!</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Once you have decided on predictors to use, you can go and create your model. </span><span class="w">
</span><span class="c1"># 1. Insert your predictors into the model below .</span><span class="w">
</span><span class="c1"># You can add more than one predictor by concatenating them with a + behind the "~":</span><span class="w">
</span><span class="c1"># 2. predict the missing data with the predict() function by passing in the model and the reduced dataset</span><span class="w">
</span><span class="c1"># 3. Plot the true data and your prediction and evaluate the model performance using the summary() function.</span><span class="w">
</span><span class="c1">#    Did the model perform well? You can try out different predictors and see which ones work best and which</span><span class="w">
</span><span class="c1">#    do not increase model performance.</span><span class="w">
</span><span class="c1">#                                       Insert here         Insert here      .....</span><span class="w">
</span><span class="c1">#                                             |                    |</span><span class="w">
</span><span class="c1">#                                             V                    V</span><span class="w">
</span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">data_site_daily_reduced</span><span class="o">$</span><span class="nb">T</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="o">&lt;</span><span class="n">FIRST</span><span class="w"> </span><span class="n">VARIABLE</span><span class="o">&gt;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="o">&lt;</span><span class="n">SECOND_VARIABLE</span><span class="o">&gt;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">...</span><span class="p">,</span><span class="w"> 
           </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_site_daily_reduced</span><span class="p">)</span><span class="w">

</span><span class="c1">#prediction = predict(...)</span><span class="w">
</span><span class="c1">#plot(data_site_daily$T, , xlab="Day", ylab="T [°C]")</span><span class="w">
</span><span class="c1">#points(prediction, col="red", pch=19)</span><span class="w">
</span><span class="c1">#legend(10,25, legend=c("true data", "modelled data"), col=c("black", "red"), pch=c(1,19))</span><span class="w">

</span><span class="c1">#summary(model)</span><span class="w">
</span></code></pre></div></div>

<h3 id="24-machine-learning-approaches-example-random-forests">2.4: Machine Learning approaches (example Random Forests)</h3>
<p>Here we will just take a quick look at a machine learning method which is commonly used for gap filling applications,
Random Forests.
The purpose is that you get an idea of how to implement such a method and at least you have seen it. We will not go
into the details of the actual method.</p>

<p>First we need an extra library for this regression:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">install.packages</span><span class="p">(</span><span class="s2">"randomForest"</span><span class="p">)</span><span class="w"> </span><span class="c1"># &lt;-- You can comment this line out once you installed the package</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">randomForest</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>When applying machine learning or generally models to actual applications, what you have to do is split your data
into two independent sets. One you will use to construct your model on, this is the so called “training” dataset.
The second dataset will be used to test your constructed model on and see how well it performs on data it has 
never seen before. This split is extremely important to maintain, because otherwise you might get an overestimation
of your model performance and your claims can easily be disproved.<br />
In the previous examples we omitted this procedure because we knew the missing data and could evaluate our model
performance on it, in real life however we do not have the missing data and need to act like a part of the present
datapoints are missing.</p>

<p>From the reduced dataset we remove the rows containing NAN, which is the data we actually do not have.</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_site_daily_reduced_noNA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_site_daily_reduced</span><span class="p">[</span><span class="n">complete.cases</span><span class="p">(</span><span class="n">data_site_daily_reduced</span><span class="p">),]</span><span class="w"> </span><span class="c1"># Remove rows containing NA</span><span class="w">
</span></code></pre></div></div>
<p>Additionally the columns containing datetime, line, DP and ST are removed because they are no good predictors or 
are derived from temperature. In the case of surface temperature, we will just act like we don’t have it to make
the prediction more interesting.</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_site_daily_reduced_noNA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_site_daily_reduced_noNA</span><span class="p">[</span><span class="m">3</span><span class="o">:</span><span class="m">18</span><span class="p">]</span><span class="w"> </span><span class="c1"># Removing line and ST</span><span class="w">
</span><span class="n">data_site_daily_reduced_noNA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_site_daily_reduced_noNA</span><span class="p">[,</span><span class="m">-14</span><span class="p">]</span><span class="w"> </span><span class="c1"># Removing ST</span><span class="w">
</span><span class="n">data_site_daily_reduced_noNA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_site_daily_reduced_noNA</span><span class="p">[,</span><span class="m">-5</span><span class="p">]</span><span class="w"> </span><span class="c1"># Removing DP</span><span class="w">
</span></code></pre></div></div>

<p>Now we will split the remaining dataset into two sets comprising of 70% of the date for training and 30% for testing
We will not use a consecutive sample of the data (e.g. the first x%) but rather a random sample. This is because
consecutive data often time contains a correlation in itself which can lead to biased models. No further details here,
just keep in mind that when training models randomizing the input is an important point (keyword “autocorrelation”)</p>

<p>For  this we can use the sample() function. We pass it all available indices of our dataset with “nrow(dataset)”. As
input you can use either a vector of values or an integer. If it is an integer like we used here, it is the values 
1 to the integer, so 1 to nrow(data_site_daily_reduced_noNA).
Then we specifiy that we want our sample to be 70% of that data with 0.7*nrow(data_site_daily_reduced_noNA).
With replace = False we specify that we want to remove the indices after sampling them, so we can not pick
an index twice in our sample.</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">data_site_daily_reduced_noNA</span><span class="p">),</span><span class="w"> </span><span class="m">0.7</span><span class="o">*</span><span class="n">nrow</span><span class="p">(</span><span class="n">data_site_daily_reduced_noNA</span><span class="p">),</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Now we use the sample of indices above to create our training and testing datasets. 
First we use the indices directly to pick the values from the original dataset into our TrainSet</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TrainSet</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data_site_daily_reduced_noNA</span><span class="p">[</span><span class="n">train</span><span class="p">,]</span><span class="w">
</span></code></pre></div></div>

<p>Then we use the same indices to create our validation dataset, by picking those indices from the original dataset
which are NOT in the train indices array by putting a minus in front of it. That is “exclusive” indexing.</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ValidSet</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data_site_daily_reduced_noNA</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,]</span><span class="w">
</span><span class="n">nrow</span><span class="p">(</span><span class="n">TrainSet</span><span class="p">)</span><span class="w">
</span><span class="n">nrow</span><span class="p">(</span><span class="n">ValidSet</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>I will only give a very brief intro to random forests here, no need to memorize that. If you are interested,
you can also look the below youtube video for a very good short video on the method.<br />
https://www.youtube.com/watch?v=v6VJ2RO66Ag</p>

<p>Very generally speaking you can say that this algorithm looks at your data and the predictors and it picks
a few of  the predictors, leaving others out.
With this reduced set it trains a model. That means, it tries to find out under which circumstances in the 
predictors, the data has a certain value. 
In random forests, many of those models are trained and compared. Each with different predictors and trained on 
different amounts and points of training data.
After building the model, you can use it to predict unknown values. Therefore, the predictor data for these 
unknown datapoints is fed into each of these models and the combined output from all of them is evaluated as the
final decision.</p>

<p>We will create a Random Forest model with mtry = 6. The mtry keyword defines, how many predictors will be considered
in each model. The argument ntree = 500 means that we will create a total number of 500 models, each containing
a different combination of predictors and data.</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rfmodel</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">randomForest</span><span class="p">(</span><span class="nb">T</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TrainSet</span><span class="p">,</span><span class="w"> </span><span class="n">importance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="o">=</span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="n">ntree</span><span class="o">=</span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"regression"</span><span class="p">)</span><span class="w">
</span><span class="n">rfmodel</span><span class="w">
</span></code></pre></div></div>

<p>Similar to how we used the predict() method before for the linear models, we can use it here on our random forest
model. We feed it the model and our validation dataset to test the model performance on unknown data:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predValid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">rfmodel</span><span class="p">,</span><span class="w"> </span><span class="n">ValidSet</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Lets take a look at how the model output looks compared to the actual data:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">ValidSet</span><span class="o">$</span><span class="nb">T</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"Day"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"T [°C]"</span><span class="p">)</span><span class="w">
</span><span class="n">points</span><span class="p">(</span><span class="n">predValid</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span><span class="w">
</span><span class="n">legend</span><span class="p">(</span><span class="m">110</span><span class="p">,</span><span class="m">26</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"true data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"modelled data"</span><span class="p">),</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">19</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>Now we calculate some metrics to evaluate our model performance:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
    </span><span class="s2">"RMSE"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="n">mean</span><span class="p">((</span><span class="n">ValidSet</span><span class="o">$</span><span class="nb">T</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">predValid</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)),</span><span class="w">
    </span><span class="s2">"R^2"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">ValidSet</span><span class="o">$</span><span class="nb">T</span><span class="p">,</span><span class="w"> </span><span class="n">predValid</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="w">
    </span><span class="p">)</span><span class="w">
</span><span class="n">metrics</span><span class="w">
</span></code></pre></div></div>

<p>Our R^2 of more than roughly 0.78 is quite satisfying, considering that we are dealing with daily averaged data in a meteorological context.
A reasonable amount (78%) of the data variance is represented by our model.
The root mean square error of 2.9 is not too bad, but does indicate that specific absolute values are not
exactly predicted by the model.</p>

<p>Finally we can use our model to predict the missing data in our original dataframe:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predgap</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">rfmodel</span><span class="p">,</span><span class="w"> </span><span class="n">data_site_daily_reduced</span><span class="p">[</span><span class="n">removed_indices</span><span class="p">,])</span><span class="w">
</span></code></pre></div></div>

<p>To compare our results, we can plot the reduced data, the true valus and our model output together:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">(</span><span class="n">data_site_daily_reduced</span><span class="o">$</span><span class="nb">T</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="o">=</span><span class="s2">"Day"</span><span class="p">,</span><span class="w"> </span><span class="n">cex</span><span class="o">=</span><span class="m">0.85</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="o">=</span><span class="s2">"T [°C]"</span><span class="p">)</span><span class="w">
</span><span class="n">points</span><span class="p">(</span><span class="n">removed_indices</span><span class="p">,</span><span class="n">predgap</span><span class="p">,</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">,</span><span class="w"> </span><span class="n">cex</span><span class="o">=</span><span class="m">0.85</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span><span class="w">

</span><span class="c1">#points(removed_indices,data_site_daily$T[removed_indices], cex=0.85, pch=19, col="black")</span><span class="w">
</span><span class="c1">#legend(1,25, legend=c("gap data", "modelled data", "true data"), col=c("black", "red", "black"), pch=c(1,19, 19))</span><span class="w">
</span><span class="n">legend</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">25</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"gap data"</span><span class="p">,</span><span class="w"> </span><span class="s2">"modelled data"</span><span class="p">),</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w"> </span><span class="n">pch</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">19</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p>Also look at the plot above with the true data plotted (remove the commenting sign in the cell above).
Now we calculate some metrics to evaluate our model performance:</p>
<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="w">
    </span><span class="s2">"RMSE"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="n">mean</span><span class="p">((</span><span class="n">data_site_daily</span><span class="o">$</span><span class="nb">T</span><span class="p">[</span><span class="n">removed_indices</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">predgap</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)),</span><span class="w">
    </span><span class="s2">"R^2"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cor</span><span class="p">(</span><span class="n">data_site_daily</span><span class="o">$</span><span class="nb">T</span><span class="p">[</span><span class="n">removed_indices</span><span class="p">],</span><span class="w"> </span><span class="n">predgap</span><span class="p">,</span><span class="w">  </span><span class="n">use</span><span class="o">=</span><span class="s2">"complete.obs"</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="w">
    </span><span class="p">)</span><span class="w">
</span><span class="n">metrics</span><span class="w">
</span></code></pre></div></div>

<p>As you can see, the R^2 of our gap filled data is not very high. We used the model to predict a gap of data that
is a lot smaller than the validation dataset. It seems that, even though the model did a pretty good job 
predicting the trend of the data on longer timescales, when we look at shorter periods the represented variance decreases
drastically.</p>

<p>Keep in mind that a model is by definition NEVER the actual truth. Its goal is to come as close as possible to the truth
while often times drastically reducing the complexity of the issue. In a real world scenario we would not
know that our final modelled data is not the same as the true data. Therefore all we can do is create and test our models
to the best of our knowledge and be honest about what they are capable of doing and what their shortcomings are!</p>

        
      </section>

      <footer class="page__meta">
        
        


        

      </footer>

      

      
    </div>

    
  </article>

  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://instagram.com/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Data Analysis for Ecologists. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>





  
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
  
    <script src="/assets/js/clipboardrougev2.js"></script>
  



  </body>
</html>
